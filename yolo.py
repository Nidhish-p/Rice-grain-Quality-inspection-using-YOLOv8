# -*- coding: utf-8 -*-
"""Yolo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SLX7NMf0BXuQZUrUgt7z1qn_L81PDfGu
"""

pip uninstall ultralytics

!pip install ultralytics==8.0.20

from IPython import display
display.clear_output()

import ultralytics
ultralytics.checks()

from ultralytics import YOLO

from IPython.display import display, Image

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

model = YOLO('yolov8n.pt')
model.train(data='/content/drive/MyDrive/yolov8/data.yaml', epochs=25)

from google.colab.patches import cv2_imshow
model = YOLO('/content/best.pt')

results=model.predict('149.png',conf=0.1)

for i in results:
  print(len(i))

print(results)

import cv2

input_image = cv2.imread('149.png')
class_counts = {}
class_names = model.names

for result in results:
    for box in result.boxes:
        cls = int(box.cls[0])
        label = class_names[cls]
        if label not in class_counts:
            class_counts[label] = 0
        class_counts[label] += 1
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        cv2.rectangle(input_image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
        cv2.putText(input_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

print("Class Counts:", class_counts)

cv2.imwrite('rice_labeled1.jpg', input_image)

!pip install opencv-python opencv-contrib-python

import cv2
import numpy as np
from ultralytics import YOLO

video_path = 'grains.mp4'
cap = cv2.VideoCapture(video_path)

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
out = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 20, (width, height))

trackers = []
tracking_objects = {}
frame_counter = 0
class_counts = {}

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    if frame_counter % 10 == 0:
        frame= cv2.GaussianBlur(frame, (5, 5), 0)
        results = model(frame, conf=0.1)

        boxes = results[0].boxes.xyxy.cpu().numpy()
        scores = results[0].boxes.conf.cpu().numpy()
        labels = results[0].boxes.cls.cpu().numpy()

        trackers.clear()
        tracking_objects.clear()

        for box, score, label in zip(boxes, scores, labels):
            x1, y1, x2, y2 = [int(coord) for coord in box]
            w, h = x2 - x1, y2 - y1


            tracker = cv2.TrackerCSRT_create()
            tracker.init(frame, (x1, y1, w, h))
            trackers.append(tracker)
            tracking_objects[label] = tracking_objects.get(label, 0) + 1

    else:
        if trackers:
            success = True
            boxes = []

            for tracker in trackers:
                success, box = tracker.update(frame)
                if success:
                    boxes.append(box)

            for i, box in enumerate(boxes):
                x, y, w, h = [int(v) for v in box]
                track_id = i
                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                cv2.putText(frame, f'Track ID: {track_id}', (x, y - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    cv2_imshow(frame)
    out.write(frame)

    frame_counter += 1
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()

print("Final Counts of Detected Grains:")
for class_name, count in tracking_objects.items():
    print(f"{class_name}: {count}")

cv2.destroyAllWindows()

!git clone https://github.com/ifzhang/ByteTrack.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/ByteTrack

!python setup.py install

pip install loguru

pip install -r requirements.txt

from google.colab.patches import cv2_imshow

pip install deep_sort_realtime

"""Different approach using trackerCSRT from openCV"""

import cv2
import numpy as np

video_capture = cv2.VideoCapture('grains.mp4')

frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(video_capture.get(cv2.CAP_PROP_FPS))

output_video = cv2.VideoWriter('tracked_grains_output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))

trackers = []
tracker_boxes = []
tracker_ids = []
class_counts = {}
class_names = model.names

def add_tracker(frame, box, grain_id):
    tracker = cv2.TrackerCSRT_create()
    tracker.init(frame, box)
    trackers.append(tracker)
    tracker_boxes.append(box)
    tracker_ids.append(grain_id)

def non_max_suppression_fast(boxes, overlap_thresh=0.3):
    if len(boxes) == 0:
        return []
    boxes = np.array(boxes)
    pick = []
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    area = (x2 - x1 + 1) * (y2 - y1 + 1)
    idxs = np.argsort(y2)
    while len(idxs) > 0:
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)
        xx1 = np.maximum(x1[i], x1[idxs[:last]])
        yy1 = np.maximum(y1[i], y1[idxs[:last]])
        xx2 = np.minimum(x2[i], x2[idxs[:last]])
        yy2 = np.minimum(y2[i], y2[idxs[:last]])
        w = np.maximum(0, xx2 - xx1 + 1)
        h = np.maximum(0, yy2 - yy1 + 1)
        overlap = (w * h) / area[idxs[:last]]
        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlap_thresh)[0])))
    return boxes[pick].astype(int)

grain_id_counter = 0

while True:
    ret, frame = video_capture.read()
    if not ret:
        break
    for i, tracker in enumerate(trackers):
        success, box = tracker.update(frame)
        if success:
            tracker_boxes[i] = box
        else:
            trackers.pop(i)
            tracker_boxes.pop(i)
            tracker_ids.pop(i)
            continue
    for i, box in enumerate(tracker_boxes):
        p1 = (int(box[0]), int(box[1]))
        p2 = (int(box[0] + box[2]), int(box[1] + box[3]))
        cv2.rectangle(frame, p1, p2, color=(0, 255, 0), thickness=2)
        class_label = class_names[tracker_ids[i] % len(class_names)]
        cv2.putText(frame, f"{class_label} ID: {tracker_ids[i]}", (p1[0], p1[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    if grain_id_counter % 10 == 0 or len(trackers) == 0:
        results = model(frame, conf=0.1)
        boxes = []
        for result in results:
            for box in result.boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                class_id = int(box.cls)
                class_label = class_names[class_id]
                boxes.append([x1, y1, x2, y2])
                if class_label not in class_counts:
                    class_counts[class_label] = 0
                if not any([cv2.norm(np.array([x1, y1, x2, y2]) - np.array(tracker_box)) < 50 for tracker_box in tracker_boxes]):
                    class_counts[class_label] += 1
        if len(boxes) > 0:
            nms_boxes = non_max_suppression_fast(boxes)
            for bbox in nms_boxes:
                x1, y1, x2, y2 = bbox
                box = (x1, y1, x2 - x1, y2 - y1)
                add_tracker(frame, box, grain_id_counter)
                grain_id_counter += 1
    output_video.write(frame)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
    grain_id_counter += 1
video_capture.release()
output_video.release()
cv2.destroyAllWindows()
print("Class Counts:", class_counts)

"""Final tracking code with about 92% accuracy using yolov8."""

import cv2
import numpy as np
from ultralytics import YOLO
from scipy.optimize import linear_sum_assignment
import math

video_path = 'grains3.mp4'
cap = cv2.VideoCapture(video_path)
model = YOLO('best.pt')

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
out = cv2.VideoWriter('output_video_mid.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 20, (width, height))

max_distance = 50
lost_frames_threshold = 5
confirmation_frames = 7
next_track_id = 0
tracked_objects = {}
tracked_grains = set()

def calculate_cost_matrix(tracked_objects, detections):
    cost_matrix = np.zeros((len(tracked_objects), len(detections)), dtype=np.float32)
    for i, (track_id, (cx, cy, _, _, _)) in enumerate(tracked_objects.items()):
        for j, (detection_cx, detection_cy, _) in enumerate(detections):
            cost_matrix[i, j] = math.hypot(detection_cx - cx, detection_cy - cy)
    return cost_matrix

def update_tracked_objects(tracked_objects, detections):
    global next_track_id
    cost_matrix = calculate_cost_matrix(tracked_objects, detections)
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    unmatched_tracks = set(tracked_objects.keys())
    unmatched_detections = set(range(len(detections)))
    matched = set()

    for r, c in zip(row_ind, col_ind):
        if cost_matrix[r, c] < max_distance:
            matched.add((r, c))
            unmatched_tracks.remove(list(tracked_objects.keys())[r])
            unmatched_detections.remove(c)

    for r, c in matched:
        track_id = list(tracked_objects.keys())[r]
        detection = detections[c]
        class_id = detection[2]
        cx, cy = detection[0], detection[1]
        tracked_objects[track_id] = (cx, cy, class_id, 0, tracked_objects[track_id][4] + 1)

        if tracked_objects[track_id][4] == confirmation_frames:
            tracked_grains.add((track_id, class_id))

    for track_id in unmatched_tracks:
        cx, cy, class_id, lost_frames, age = tracked_objects[track_id]
        lost_frames += 1
        tracked_objects[track_id] = (cx, cy, class_id, lost_frames, age)
        if lost_frames >= lost_frames_threshold:
            del tracked_objects[track_id]

    for detection_index in unmatched_detections:
        detection_cx, detection_cy, class_id = detections[detection_index]
        is_new_grain = all(
            math.hypot(detection_cx - cx, detection_cy - cy) >= max_distance
            for cx, cy, _, _, _ in tracked_objects.values()
        )
        if is_new_grain:
            tracked_objects[next_track_id] = (detection_cx, detection_cy, class_id, 0, 1)
            next_track_id += 1

bottom_threshold = int(height * 0.65)
inactive_grains = set()
frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frame_count += 1
    results = model(frame)
    boxes = results[0].boxes.xyxy.cpu().numpy()
    labels = results[0].boxes.cls.cpu().numpy()

    detections = []
    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = map(int, box)
        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
        label = int(labels[i])
        detections.append((cx, cy, label))
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

    update_tracked_objects(tracked_objects, detections)

    for track_id, (cx, cy, class_id, lost_frames, age) in list(tracked_objects.items()):
        if cy > bottom_threshold:
            inactive_grains.add(track_id)
            del tracked_objects[track_id]

    for track_id, (cx, cy, class_id, lost_frames, age) in tracked_objects.items():
        if track_id in inactive_grains:
            continue

        cv2.circle(frame, (int(cx), int(cy)), 5, (0, 0, 255), -1)
        cv2.putText(frame, f'ID:{track_id}', (int(cx), int(cy) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
        if age >= confirmation_frames:
            cv2.putText(frame, f'Class:{class_id}', (int(cx), int(cy) + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    out.write(frame)

cap.release()
out.release()

class_counts = {}
for _, class_id in tracked_grains:
    class_counts[class_id] = class_counts.get(class_id, 0) + 1

print("Grain counts by class:")
for class_id, count in class_counts.items():
    print(f"Class {class_id}: {count}")